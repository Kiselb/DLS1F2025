{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "279be496",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install face_recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfc5725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "IMG_DIR = 'img_celeba'  # Папка с оригинальными фото (Wild)\n",
    "SAVE_DIR = 'celeba_cropped_10k'\n",
    "ALIGNED_DIR = 'celeba_aligned'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74065316",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.skip = nn.Identity() if in_channels == out_channels else nn.Conv2d(in_channels, out_channels, 1)\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels // 2, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels // 2)\n",
    "        self.conv2 = nn.Conv2d(out_channels // 2, out_channels // 2, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels // 2)\n",
    "        self.conv3 = nn.Conv2d(out_channels // 2, out_channels, 1)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.bn3(self.conv3(self.relu(self.bn2(self.conv2(self.relu(self.bn1(self.conv1(x)))))))) + self.skip(x))\n",
    "\n",
    "class Hourglass(nn.Module):\n",
    "    def __init__(self, depth, channels):\n",
    "        super().__init__()\n",
    "        self.depth = depth\n",
    "        self.up = ResidualBlock(channels, channels)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.low = Hourglass(depth - 1, channels) if depth > 1 else ResidualBlock(channels, channels)\n",
    "        self.low_post = ResidualBlock(channels, channels)\n",
    "        self.up_sample = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.up(x) + self.up_sample(self.low_post(self.low(self.pool(x))))\n",
    "\n",
    "class StackedHourglass(nn.Module):\n",
    "    def __init__(self, num_stacks=2, num_pts=5, channels=128):\n",
    "        super().__init__()\n",
    "        self.num_stacks = num_stacks\n",
    "        self.pre = nn.Sequential(nn.Conv2d(3, 64, 7, stride=2, padding=3), nn.BatchNorm2d(64), nn.ReLU(True),\n",
    "                                 ResidualBlock(64, 128), nn.MaxPool2d(2, 2), ResidualBlock(128, channels))\n",
    "        self.hgs = nn.ModuleList([Hourglass(4, channels) for _ in range(num_stacks)])\n",
    "        self.features = nn.ModuleList([nn.Sequential(ResidualBlock(channels, channels), nn.Conv2d(channels, channels, 1),\n",
    "                                                    nn.BatchNorm2d(channels), nn.ReLU(True)) for _ in range(num_stacks)])\n",
    "        self.outs = nn.ModuleList([nn.Conv2d(channels, num_pts, 1) for _ in range(num_stacks)])\n",
    "        self.merge_preds = nn.ModuleList([nn.Conv2d(num_pts, channels, 1) for _ in range(num_stacks-1)])\n",
    "        self.merge_feats = nn.ModuleList([nn.Conv2d(channels, channels, 1) for _ in range(num_stacks-1)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pre(x)\n",
    "        outputs = []\n",
    "        for i in range(self.num_stacks):\n",
    "            hg = self.hgs[i](x)\n",
    "            feature = self.features[i](hg)\n",
    "            preds = self.outs[i](feature)\n",
    "            outputs.append(preds)\n",
    "            if i < self.num_stacks - 1:\n",
    "                x = x + self.merge_feats[i](feature) + self.merge_preds[i](preds)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c0a615",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "from torchvision.models import ResNet18_Weights\n",
    "\n",
    "class FaceNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.backbone = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "        \n",
    "        self.in_features = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Identity() # Убираем стандартный классификатор\n",
    "        \n",
    "        # Слой эмбеддингов (промежуточный вектор признаков лица) для ArcFace\n",
    "        self.embedding_layer = nn.Linear(self.in_features, 128)\n",
    "        \n",
    "        # Финальный слой для Cross-Entropy (в ArcFace он не используется напрямую)\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x, return_embedding=False):\n",
    "        feat = self.backbone(x)\n",
    "        embed = self.embedding_layer(feat)\n",
    "        \n",
    "        # Для ArcFace нам нужны нормализованные эмбеддинги\n",
    "        if return_embedding:\n",
    "            return F.normalize(embed, p=2, dim=1)\n",
    "        \n",
    "        # Для Cross-Entropy возвращаем логиты\n",
    "        return self.fc(embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d38b409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Face Alignment Функция - копия\n",
    "def align_face(img, pts):\n",
    "    # Эталонные точки (глаза и нос)\n",
    "    dst = np.array([[35, 45], [93, 45], [64, 75]], dtype=np.float32)\n",
    "    src = pts[:3].astype(np.float32)\n",
    "    M = cv2.getAffineTransform(src, dst)\n",
    "    return cv2.warpAffine(img, M, (128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e268f922",
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition\n",
    "\n",
    "class FaceRecognitionPipeline:\n",
    "    def __init__(self, hourglass_path, facenet_path, num_classes, device='cpu'):\n",
    "        self.device = device\n",
    "        \n",
    "        # Hourglass (из Задания 1)\n",
    "        self.hourglass = StackedHourglass(num_stacks=2).to(device)\n",
    "        self.hourglass.load_state_dict(torch.load(hourglass_path, map_location=device))\n",
    "        self.hourglass.eval()\n",
    "        \n",
    "        # FaceNet (из Задания 2)\n",
    "        self.facenet = FaceNet(num_classes=num_classes).to(device)\n",
    "        self.facenet.load_state_dict(torch.load(facenet_path, map_location=device))\n",
    "        self.facenet.eval()\n",
    "\n",
    "    def process_image(self, img_path):\n",
    "        image = cv2.imread(img_path)\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Детекция (предобученная модель)\n",
    "        face_locations = face_recognition.face_locations(image_rgb)\n",
    "        \n",
    "        results = []\n",
    "        for (top, right, bottom, left) in face_locations:\n",
    "            # Кроп лица для Hourglass\n",
    "            face_crop = image[top:bottom, left:right]\n",
    "            face_resize = cv2.resize(face_crop, (128, 128))\n",
    "            \n",
    "            # Выравнивание (Hourglass)\n",
    "            input_t = torch.tensor(cv2.cvtColor(face_resize, cv2.COLOR_BGR2RGB)).permute(2,0,1).float().unsqueeze(0).to(self.device)/255.\n",
    "            with torch.no_grad():\n",
    "                hms = self.hourglass(input_t)[-1].cpu().numpy()[0]\n",
    "            \n",
    "            pts = []\n",
    "            for i in range(5):\n",
    "                y, x = np.unravel_index(np.argmax(hms[i]), (32, 32))\n",
    "                pts.append([x * 4, y * 4])\n",
    "            \n",
    "            aligned_face = align_face(face_resize, np.array(pts))\n",
    "            \n",
    "            # Эмбеддинг (FaceNet)\n",
    "            # Нормализация ImageNet\n",
    "            transform = transforms.Compose([\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "            face_t = transform(cv2.cvtColor(aligned_face, cv2.COLOR_BGR2RGB)).unsqueeze(0).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                embedding = self.facenet(face_t, return_embedding=True)\n",
    "            \n",
    "            results.append({\n",
    "                'bbox': (top, right, bottom, left),\n",
    "                'embedding': embedding.cpu().numpy()[0],\n",
    "                'aligned_face': aligned_face\n",
    "            })\n",
    "            \n",
    "        return results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
